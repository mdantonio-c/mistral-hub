#!/usr/bin/python3

# Copyright (C) 2020  Paolo Patruno <p.patruno@iperbole.bologna.it>
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
#
# Authors: Paolo Patruno <p.patruno@iperbole.bologna.it>

import argparse
import datetime
import errno
import logging
import os
import shutil
import subprocess
import sys
import tempfile

import dballe
from mistral.services.arkimet import BeArkimet
from mistral.services.dballe import BeDballe
from mistral.services.sqlapi_db_manager import SqlApiDbManager
from restapi.config import get_backend_url
from restapi.connectors import smtp, sqlalchemy

_version = 1.0
stderr = sys.stderr
stdout = sys.stdout

default_date = (datetime.datetime.utcnow() - datetime.timedelta(days=7)).strftime(
    "%Y-%m-%d"
)

parser = argparse.ArgumentParser(description="Migrate data from dballe to arkimet.")
parser.add_argument("--version", action="version", version=f"%(prog)s {_version}")
parser.add_argument(
    "-a",
    "--arkiconf",
    dest="arkiconf",
    action="store",
    default="/rmap/arkimet/arkimet.conf",
    help="arkiserver to contact. Default: %(default)s",
)
parser.add_argument(
    "-d",
    "--dsn",
    dest="dsn",
    action="store",
    default="mysql:///rmap?user=rmap&password=rmap",
    help="arkiserver to contact. Default: %(default)s",
)
parser.add_argument(
    "-o",
    "--outputdir",
    dest="outputdir",
    action="store",
    default="/tmp/" + "/dballe2arkimet",
    help="output directory where to write data. Default: %(default)s",
)
parser.add_argument(
    "-f",
    "--file",
    dest="outputfile",
    action="store",
    default="dballe2arkimet.dat",
    help="output file name for data. Default: %(default)s",
)
parser.add_argument(
    "-e",
    "--date",
    dest="date",
    action="store",
    default=default_date,
    help="reference date (AAAA-MM-DD). Default: <today-7days>",
)
# parser.add_argument('-t', '--time', dest='time', action='store',
#                    default="00:00:00",
#                    help='reference time (HH:MM:SS). Default: %(default)s')
parser.add_argument(
    "-p",
    "--tempprefix",
    dest="tempprefix",
    action="store",
    default="/tmp/",
    help="Prefix for temporary work directory. Default: %(default)s",
)
parser.add_argument(
    "--cachedir",
    metavar="dir",
    action="store",
    type=str,
    default=None,
    help="cache directory for warped channels. Default: %(default)s",
)
parser.add_argument(
    "-v",
    "--verbose",
    action="store",
    type=bool,
    default=False,
    help="set verbosity level to DEBUG, Default: %(default)s",
)

opts = parser.parse_args()

date = datetime.date(*[int(e) for e in opts.date.split("-")])


class Error(Exception):
    """Base class for exceptions in this module."""

    pass


class TmpDirError(Error):
    """Exception raised setting temporary working dir.
    Attributes:
        expr -- input expression in which the error occurred
        msg  -- explanation of the error
    """

    def __init__(self, expr, msg):
        self.expr = expr
        self.msg = msg

    def __str__(self):
        return repr(self.msg) + "Dir: " + repr(self.expr)


class makeenv:
    def __init__(self, tempprefix=None):

        self.cwd = None

        if tempprefix is not None:
            # tempfile.tempdir=tempprefix
            tmp = tempfile.mkdtemp(prefix=tempprefix)
        else:
            tmp = tempfile.mkdtemp(prefix="/tmp/")

        logging.info("Working temporary directory: " + tmp)
        os.chdir(tmp)

        if not os.path.samefile(tmp, os.getcwd()):
            raise TmpDirError(
                (tmp, os.getcwd()), "Error testing cwd after chdir in tmp working dir"
            )
        else:
            self.cwd = tmp

        # prepare output directory
        try:
            os.makedirs(opts.outputdir)
        except OSError as exc:  # Python >2.5
            if exc.errno == errno.EEXIST and os.path.isdir(opts.outputdir):
                pass
            else:
                raise

    def delete(self):

        dangerouspaths = ("/", "/home", "/home/")

        if self.cwd is not None and self.cwd not in dangerouspaths:
            # print "remove working tree ",self.cwd
            shutil.rmtree(self.cwd)
        else:
            logging.info(
                "tempprefix is a dangerous path: do not remove temporary working directoy"
            )


def dballe2arkimet(date):
    logging.info("Start to migrate data to arkimet")
    rec = {"yearmax": date.year, "monthmax": date.month, "daymax": date.day}
    user = os.environ.get("ALCHEMY_USER")
    pw = os.environ.get("ALCHEMY_PASSWORD")
    host = os.environ.get("ALCHEMY_HOST")
    engine = os.environ.get("ALCHEMY_DBTYPE")
    port = os.environ.get("ALCHEMY_PORT")
    # get the list of dsn from the db
    alchemy_db = sqlalchemy.get_instance()
    license_groups = alchemy_db.GroupLicense.query.filter_by().all()
    dsn_list = []
    dsn_list.extend(
        x.dballe_dsn
        for x in license_groups
        if x.dballe_dsn and x.dballe_dsn not in dsn_list
    )
    datasets_to_filter = [
        BeArkimet.from_network_to_dataset(x) for x in BeDballe.MAPS_NETWORK_FILTER
    ]
    license_groups_need_filtering = []
    for ds in datasets_to_filter:
        group_lic = SqlApiDbManager.get_license_group(alchemy_db, [ds])
        license_groups_need_filtering.append(group_lic.name)
    for dsn in dsn_list:
        logging.info(f"Start migrating data from {dsn}")
        db = dballe.DB.connect(f"{engine}://{user}:{pw}@{host}:{port}/{dsn}")
        migrate_data(db, dsn, rec, alchemy_db, license_groups_need_filtering)
        try:
            mobile_dsn = f"{dsn}_MOBILE"
            mobile_db = dballe.DB.connect(
                f"{engine}://{user}:{pw}@{host}:{port}/{mobile_dsn}"
            )
        except OSError:
            logging.info(f"{dsn} dsn for mobile station data does not exists")
            mobile_db = None
        if mobile_db:
            migrate_data(mobile_db, dsn, rec, alchemy_db, license_groups_need_filtering)


def migrate_data(db, dsn, rec, alchemy_db, license_groups_need_filtering):
    tmpdatafile = "tmpdatafile"
    with db.transaction() as tr:
        exporter = dballe.Exporter("BUFR")
        with open(tmpdatafile, "wb") as tmpfile:
            for row in tr.query_messages(rec):
                tmpfile.write(exporter.to_binary(row.message))
        # ---------------- UPDATE JSON SUMMARY --------------------

        dsn_license_group_list = [
            x.name
            for x in alchemy_db.GroupLicense.query.filter_by(dballe_dsn=dsn).all()
        ]
        if len(dsn_license_group_list) == 1:
            # path to json_summary_file
            json_summary = "{}_{}.json".format(
                BeDballe.ARKI_JSON_SUMMARY_PATH,
                dsn_license_group_list[0].replace(" ", "_"),
            )
            general_explorer = dballe.DBExplorer()

            with general_explorer.update() as updater:
                # Load existing json summary
                if os.path.exists(json_summary):
                    with open(json_summary) as fd:
                        updater.add_json(fd.read())

                # Import files listed on command line
                importer = dballe.Importer("BUFR")
                with importer.from_file(tmpdatafile) as f:
                    # updater.add_messages(f)
                    # for the new version
                    updater.add_messages(f, station_data=False, data=True)

            # Write out the general summary
            with open(json_summary, "wt") as fd:
                fd.write(general_explorer.to_json())

            # check if the dsn needs a filtered summary (multimodel use case)
            if dsn_license_group_list[0] in license_groups_need_filtering:
                json_summary_filtered = "{}_{}.json".format(
                    BeDballe.ARKI_JSON_SUMMARY_PATH_FILTERED,
                    dsn_license_group_list[0].replace(" ", "_"),
                )
                # create the filtered summary
                all_nets = general_explorer.all_reports
                filtered_explorer = dballe.DBExplorer()
                for net in all_nets:
                    if net not in BeDballe.MAPS_NETWORK_FILTER:
                        general_explorer.set_filter({"rep_memo": net})
                        # add the filtered explorer to the general one
                        with filtered_explorer.update() as updater:
                            updater.add_explorer(general_explorer)

                # export the filtered explorer to a json file
                with open(json_summary_filtered, "wt") as fd:
                    fd.write(filtered_explorer.to_json())
        else:
            # case of dsn containing different license groups
            for lg in dsn_license_group_list:
                # path to json_summary_file
                json_summary = "{}_{}.json".format(
                    BeDballe.ARKI_JSON_SUMMARY_PATH_FILTERED, lg.replace(" ", "_")
                )
                general_explorer = dballe.DBExplorer()
                # create an explorer for the existing json summary+the new data to be imported
                with general_explorer.update() as updater:
                    # Load existing json summary
                    if os.path.exists(json_summary):
                        with open(json_summary) as fd:
                            updater.add_json(fd.read())
                    # Import files listed on command line
                    importer = dballe.Importer("BUFR")
                    with importer.from_file(tmpdatafile) as f:
                        # updater.add_messages(f)
                        # for the new version
                        updater.add_messages(f, station_data=False, data=True)

                # retrieve networks of the license group
                dataset_list = SqlApiDbManager.retrieve_dataset_by_license_group(
                    alchemy_db, lg
                )
                net_list = []
                for ds in dataset_list:
                    nets = BeArkimet.from_dataset_to_networks(ds)
                    net_list.extend(n for n in nets)

                # create a subset of the explorer
                subset_explorer = dballe.DBExplorer()
                for n in net_list:
                    general_explorer.set_filter({"rep_memo": n})
                    with subset_explorer.update() as updater:
                        updater.add_explorer(general_explorer)

                # write the filtered explorer to the file
                with open(json_summary, "wt") as fd:
                    fd.write(subset_explorer.to_json())

        # ---------------------------------------------------------

        returncode = subprocess.call(
            (
                "arki-scan",
                "--dispatch=" + opts.arkiconf,
                "bufr:" + tmpdatafile,
                "--summary",
                "--dump",
                "--status",
            ),
            stderr=stderr,
            stdout=stdout,
        )

        logging.info("End to migrate data to arkimet")
        logging.info("Return status: %s" % (returncode))

        if returncode == 0 or returncode == 2:
            logging.info("Start to delete data from dballe")
            tr.remove_data(rec)
            logging.info("End to delete data from dballe")
        else:
            tr.rollback()
            logging.error("Error migrate data from dballe to arkimet")
            logging.warning("Do not delete data from dballe")
            # sent alert by mail
            smtp_client = smtp.get_instance()
            host = get_backend_url()
            smtp_client.send(
                "Error migrate data from dballe to arkimet: data not deleted from DBalle",
                f"Alert from {host} : Migration error",
                to_address="b.chiavarini@cineca.it",
            )


def main():
    # Get an instance of a logger

    # we want work like fortran ? TODO
    #  call getenv("LOG4_APPLICATION_NAME",LOG4_APPLICATION_NAME)
    #  call getenv("LOG4_APPLICATION_ID",LOG4_APPLICATION_ID)

    if opts.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    logger = logging.getLogger(__name__)

    logger.info("start")
    try:
        logger.info("makeenv")
        env = makeenv(tempprefix=opts.tempprefix)
    except TmpDirError as e:
        logger.exception(e)
        raise
    except BaseException:
        logger.exception("premature end")
        raise

    if opts.cachedir is None:
        opts.cachedir = env.cwd

    try:
        logger.info("start dballe2arkimet")
        dballe2arkimet(date)

    except BaseException:
        logger.exception("Error happen")
        raise
    finally:
        env.delete()
        logger.info("end")


if __name__ == "__main__":
    stdout.write(sys.argv[0] + " started with pid %d\n" % os.getpid())
    stdout.write(sys.argv[0] + " stdout output\n")
    stderr.write(sys.argv[0] + " stderr output\n")

    # (this code was run as script)
    sys.exit(main())
